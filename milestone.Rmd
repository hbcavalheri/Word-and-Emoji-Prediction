---
title: "Peer-graded Assignment: Milestone Report"
author: Hamanda Cavalheri
date: February 18, 2018
output: html_document
---

```{r setup, include=FALSE, cache = TRUE}
knitr::opts_chunk$set(echo = TRUE)
```

## Goal

This milestone project is part of the Coursera Data Science Specialization. The goal of this project is to perform exploratory analysis to show the major features of the data and briefly summarize the plans for the prediction algorithm and Shiny app.

I used the library 'tidytext' as a base to analyse and process the data. Most of the analyses and graphs that I show I learned reading the 'Text Mining with R' book by Julia Silge and David Robison. For speed purposes, I randomly selected 5000 lines of each dataset in English.

## Downloading the data

```{r, cache = TRUE, warning= FALSE, message=FALSE}
library(knitr)
library(tidytext)
library(tidyverse)
library(downloader)
library(gridExtra)
library(dplyr)
library(reshape2)
library(wordcloud)
```

```{r, eval = FALSE, echo = TRUE, result = 'hide'}
fileURL<-"https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
download(fileURL, dest="final", mode="wb") 
unzip("final.zip")
my_dirs <- list.files("/Users/hamandacavalheri/Desktop/Coursera_Data Science/capstone_project", pattern = '.txt', recursive = TRUE, include.dirs = TRUE)
new_dir <- "/Users/hamandacavalheri/Desktop/Coursera_Data Science/capstone_project/"
files2 <- paste(new_dir, my_dirs, sep = "")
for(i in 1:length(files2)){
    file.copy(files2[i], to = new_dir)
}
```

```{r, cache = TRUE, warning = FALSE, eval = TRUE, message=FALSE}

us.blog <- readLines('en_US.blogs.txt')
us.news <- readLines('en_US.news.txt')
us.twi <- readLines('en_US.twitter.txt')
```

Summary of the datasets.
```{r, echo = FALSE, cache = TRUE}
sum.data <- data.frame('Data' = c('Blog', 'News', 'Twitter'),
                       'File size (Mb)' = 
                           c(round(260564320 / 1024^2, 2),
                             round(261759048 / 1024^2, 2),
                             round(316037344 / 1024^2, 2)),
                       'Number of lines' = c(length(us.blog), length(us.news),
                       length(us.twi)))
print(sum.data)
```

Randomly selecting 5000 lines from the each dataset.

```{r, include = TRUE}
us.blog <- sample(us.blog, 5000)
us.news <- sample(us.news, 5000)
us.twi <- sample(us.twi, 5000)
```

## Tokenization

Here, I show how to process tokenization, using as a token each word, i.e. an unigram. I also filter the datasets to remove words that are common and not useful for the analysis, such as 'the', 'of', 'to'. 

Figures 1, 2 and 3 show the words with the highest frequencies.

Here is an example for the Blog dataset.
```{r, cache = TRUE, warning = FALSE, message=FALSE}
us.blog1 <- us.blog %>% 
    data_frame(line = 1:length(.)) %>% 
    `colnames<-` (c("text", "line")) %>% 
    unnest_tokens(word, text) %>% 
    anti_join(stop_words) %>% 
    filter(!str_detect(word, "[0-9]")) %>% 
    count(word, sort = TRUE)
```

```{r, echo = FALSE, cache = TRUE, message=FALSE, fig.align='center'}
us.news1 <- us.news %>% 
    data_frame(line = 1:length(.)) %>% 
    `colnames<-` (c("text", "line")) %>% 
    unnest_tokens(word, text) %>% 
    anti_join(stop_words) %>% 
    filter(!str_detect(word, "[0-9]")) %>% 
    count(word, sort = TRUE)

us.twi1 <- us.twi %>% 
    data_frame(line = 1:length(.)) %>% 
    `colnames<-` (c("text", "line")) %>% 
    unnest_tokens(word, text) %>% 
    anti_join(stop_words) %>% 
    filter(!str_detect(word, "[0-9]")) %>% 
    count(word, sort = TRUE)
    
blog1 <- us.blog1 %>% 
    filter(n > 100) %>% 
    mutate(word = reorder(word, n)) %>% 
    ggplot() +
    geom_col(aes(word, n), color = 'orchid', fill = 'orchid') +
    xlab(NULL) +
    coord_flip() +
    ggtitle('Blog') +
    theme(panel.background = element_blank(),
          axis.line = element_line('black'),
          plot.title = element_text(hjust = 0.5))

news1 <- us.news1 %>% 
    filter(n > 100) %>% 
    mutate(word = reorder(word, n)) %>% 
    ggplot() +
    geom_col(aes(word, n), color = 'blue', fill = 'blue') +
    xlab(NULL) +
    coord_flip() +
    ggtitle('News') +
    theme(panel.background = element_blank(),
          axis.line = element_line('black'),
          plot.title = element_text(hjust = 0.5))

twi1 <- us.twi1 %>% 
    filter(n > 70) %>% 
    mutate(word = reorder(word, n)) %>% 
    ggplot() +
    geom_col(aes(word, n), color = 'orange', fill = 'orange') +
    xlab(NULL) +
    coord_flip() +
    ggtitle('Twitter') +
    theme(panel.background = element_blank(),
          axis.line = element_line('black'),
          plot.title = element_text(hjust = 0.5))

grid.arrange(blog1, news1, twi1, ncol = 3)

```

# Sentiment analysis on unigrams

There are dictionaries that evaluate and asign emotion to a text. Here, I am using the 'bing' dictionary found in the datasets 'sentiments' from 'tidytext' package. 

```{r, warning = FALSE, message=FALSE, fig.align='center'}
library(dplyr)
library(tidytext)
library(reshape2)
library(wordcloud)

par(mfrow = c(1,3))

us.blog1 %>% 
    inner_join(get_sentiments('bing')) %>% 
    acast(word ~ sentiment, value.var = 'n', fill = 0) %>% 
    comparison.cloud(colors = c('darkgreen', 'orange'), 
                           max.words = 100, title.size = 2)
mtext('Blog', side = 3, line = -3, cex = 2)

us.news1 %>% 
    inner_join(get_sentiments('bing')) %>% 
    acast(word ~ sentiment, value.var = 'n', fill = 0) %>% 
    comparison.cloud(colors = c('darkgreen', 'orange'), 
                           max.words = 100, title.size = 2)
mtext('News', side = 3, line = -3, cex = 2)

us.twi1 %>% 
    inner_join(get_sentiments('bing')) %>% 
    acast(word ~ sentiment, value.var = 'n', fill = 0) %>% 
    comparison.cloud(colors = c('darkgreen', 'orange'), 
                           max.words = 100, title.size = 2)
mtext('Twitter', line = -3, cex = 2)
```

# Analysing bigrams

Now, I analyze the datasets using bigrams and look at the frequency of the most commom bigrams in each dataset.

Here is an example of code for the Blog dataset.
```{r, cache = TRUE, message=FALSE, result = 'hide'}
library(dplyr)
us.blog.bi <- us.blog %>%
    data_frame(line = 1:length(.)) %>% 
    `colnames<-` (c("text", "line")) %>% 
    unnest_tokens(bigram, text, token = 'ngrams', n = 2) %>% 
    count(bigram, sort = TRUE) %>% 
    separate(bigram, c('word1', 'word2'), sep = " ") %>% 
    filter(!word1 %in% stop_words$word) %>%
    filter(!word2 %in% stop_words$word) %>% 
    filter(!str_detect(word1, "[0-9]")) %>% 
    filter(!str_detect(word2, "[0-9]")) 
```

So far I looked at the term frequency (tf) of each unigram. For the bigrams I looked at the term's inverse document frequency (idf). The idf increases the weight of the terms that are not commonly used. Thus, tf-idf is a mesurement of how important a term is in that particular dataset. It shows the terms that distinguishes one dataset from the other. The same code and analysis can be performed for trigrams.

```{r, echo = FALSE, message = FALSE, warning = FALSE, result = 'hide', fig.align='center'}
us.news.bi <- us.news %>%
    data_frame(line = 1:length(.)) %>% 
    `colnames<-` (c("text", "line")) %>% 
    unnest_tokens(bigram, text, token = 'ngrams', n = 2) %>% 
    count(bigram, sort = TRUE) %>% 
    separate(bigram, c('word1', 'word2'), sep = " ") %>% 
    filter(!word1 %in% stop_words$word) %>%
    filter(!word2 %in% stop_words$word) %>% 
    filter(!str_detect(word1, "[0-9]")) %>% 
    filter(!str_detect(word2, "[0-9]")) 

us.twi.bi <- us.twi %>%
    data_frame(line = 1:length(.)) %>% 
    `colnames<-` (c("text", "line")) %>% 
    unnest_tokens(bigram, text, token = 'ngrams', n = 2) %>% 
    count(bigram, sort = TRUE) %>% 
    separate(bigram, c('word1', 'word2'), sep = " ") %>% 
    filter(!word1 %in% stop_words$word) %>%
    filter(!word2 %in% stop_words$word) %>% 
    filter(!str_detect(word1, "[0-9]")) %>% 
    filter(!str_detect(word2, "[0-9]"))

bi.tf.idf <- bind_rows(mutate(us.blog.bi, type = 'blog'),
                       mutate(us.news.bi, type = 'news'),
                       mutate(us.twi.bi, type = 'twitter')) %>% 
    unite(bigram, word1, word2, sep = " ") %>% 
    bind_tf_idf(bigram, type, n) %>% 
    arrange(desc(tf_idf))  

bi.tf.idf %>% 
    mutate(bigram = factor(bigram, levels = rev(unique(bigram)))) %>% 
    group_by(type) %>% 
    top_n(10, tf_idf) %>% 
    ungroup() %>% 
    ggplot() +
    geom_bar(aes(bigram, tf_idf, color = type, fill = type), 
             show.legend = FALSE, stat = 'identity') +
    scale_fill_manual(values = c('blog' = 'orchid', 'news' = 'blue', 
                                 'twitter' = 'orange')) +
    scale_color_manual(values = c('blog' = 'orchid', 'news' = 'blue', 
                                 'twitter' = 'orange')) +
    coord_flip() +
    facet_wrap(~ type, scales = 'free') +
    theme(panel.background = element_blank(),
          axis.line = element_line('black'))
```

# Future directions

I intend to create a plataform on Shiny that is simple and takes as an input the user text and gives as output options of possible words that the user would want to write next. I will probably use tf-idf and word frequency to calculate the probability for each word to be used next by the user. The algoritm could be fancier and that adjust the probabilities based on user's choice.

I also want to offer emojis as an output for the user. Since communication now is moving towards pictures instead of text, I want to use sentiment analysis to evaluate user's feelings based on the input provided and offer emoji options that would correspond to the feeling detected in the text. Here: http://unicode.org/emoji/charts/full-emoji-list.html, there is encoded emojis that can be used in R.